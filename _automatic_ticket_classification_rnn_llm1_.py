# -*- coding: utf-8 -*-
"""_Automatic_Ticket_Classification_RNN_LLM1_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZFerGEhsY1GCQ0atNn_2tjtoCSAv1htN
"""

from datasets import load_dataset
import pandas as pd
import numpy as np
import random

import matplotlib.pyplot as plt
import seaborn as sns


import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow. keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional


from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam


from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.models import load_model



import os
import json
import time

# Gemini API
import google.generativeai as genai

# Set a random seed for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

#  Load the Dataset
print("Loading the dataset: Tobi-Bueck/customer-support-tickets...")

try:
    ds = load_dataset("Tobi-Bueck/customer-support-tickets")
    print("Dataset loaded successfully.")

    # Convert train split to DataFrame
    df = ds["train"].to_pandas()

    # Verify required columns
    required_columns = ["body", "queue"]
    for col in required_columns:
        if col not in df.columns:
            raise ValueError(f"Required column '{col}' not found in dataset.")

    print("Required columns verified:", required_columns)
    print("Dataset shape (train):", df.shape)

except Exception as e:
    print(f"Error loading dataset: {e}")
    raise

"""# **1.Dataset Exploration**"""

print("\nBasic Dataset Information ")

# Dataset size
print(f"Total number of records in training data: {len(df)}")

# Display input and target columns
print("\nFirst 5 records (Input: body, Target: queue):")
print(df[["body", "queue"]].head())

#  Explore Data Types
print("\nData Types of Key Fields ")
df.info()

#   Unique Queue Counts (Target Variable Analysis)
print("\nTarget Variable Analysis: 'queue' Distribution ")
queue_counts = df['queue'].value_counts()
num_classes = len(queue_counts)

print(f"Number of unique classes (queues): {num_classes}")
print("\nTop 10 Queue Distribution:")
print(queue_counts.head(10))

# Visualize the class distribution to check for imbalance
plt.figure(figsize=(12, 6))
queue_counts.plot(kind='bar', color='skyblue')
plt.title('Distribution of Customer Support Ticket Queues (Target Classes)')
plt.xlabel('Queue / Department')
plt.ylabel('Number of Tickets')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

#  Text Length Distribution (Input Variable Analysis)
print("\n Input Variable Analysis: 'body' Text Length ")

# Calculate the length of each ticket body in terms of words
df['body_length'] = df['body'].apply(lambda x: len(str(x).split()))

# Get descriptive statistics
length_stats = df['body_length'].describe(percentiles=[.50, .75, .90, .95, .99])
print("Body Text Length (in number of words) Statistics:")
print(length_stats)

# Visualize the text length distribution
plt.figure(figsize=(10, 5))
plt.hist(df['body_length'], bins=50, color='lightcoral', edgecolor='black', range=(0, length_stats['99%'] if length_stats['99%'] < 500 else 500)) # Cap at 500 or 99th percentile for better visualization
plt.title('Distribution of Ticket Body Lengths (in words)')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')

df["text_length"] = df["body"].apply(lambda x: len(str(x).split()))

# Descriptive statistics including 95th percentile
length_stats = df["text_length"].describe(percentiles=[0.95])

print("\nText Length Statistics:")
print(length_stats)

# Plot text length distribution
plt.figure(figsize=(8,5))
plt.hist(df["text_length"], bins=50)
plt.title("Distribution of Ticket Text Lengths")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")

# Mark the 95th percentile (used for MAX_LEN)
max_len_95 = int(length_stats["95%"])
plt.axvline(
    max_len_95,
    linestyle="dashed",
    linewidth=2,
    label=f"95th Percentile ({max_len_95} words)"
)

plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

print(f"Selected MAX_LEN for padding: {max_len_95}")

"""# **2.Preprocessing, label encoding.**"""

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"[^a-z\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

print(" Text Preprocessing ")
df["clean_body"] = df["body"].apply(clean_text)

sample_index = 0
print("Original Text:", df["body"].iloc[sample_index])
print("Cleaned Text:", df["clean_body"].iloc[sample_index])

# Label Encoding for Queue

print("\nLabel Encoding for Queue ")

label_encoder = LabelEncoder()
df["encoded_queue"] = label_encoder.fit_transform(df["queue"])

QUEUE_MAPPING = dict(
    zip(label_encoder.classes_, range(len(label_encoder.classes_)))
)

NUM_CLASSES = len(label_encoder.classes_)

print(f"Total number of classes: {NUM_CLASSES}")
print("Label to Index Mapping Sample:", list(QUEUE_MAPPING.items())[:5])

# Tokenization and Sequence Preparation

print("\nTokenization and Padding/Truncation ")

MAX_WORDS = 15000  # Vocabulary size: consider the top 15,000 most frequent words
MAX_LEN = 102      # Sequence length: set to ~95th percentile word count

# Initialize Keras Tokenizer
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
# Fit on the cleaned text data
tokenizer.fit_on_texts(df['clean_body'])

# Convert text to sequences of integers
sequences = tokenizer.texts_to_sequences(df['clean_body'])

# Pad/Truncate sequences to MAX_LEN
X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')
y = df['encoded_queue'].values  # The target array

VOCAB_SIZE = min(MAX_WORDS, len(tokenizer.word_index) + 1)  # +1 for the 0 padding

print(f"Vocabulary Size (Unique Tokens): {VOCAB_SIZE}")
print(f"Maximum Sequence Length (MAX_LEN): {MAX_LEN}")
print("Shape of Final Input (X):", X.shape)

# Split Data into Train, Validation, and Test Sets

print("\nData Splitting (70/15/15) ")

#  Split into Training (70%) and Temporary Test (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

#  Split the Temporary Test (30%) into Validation (15%) and Final Test (15%)
# 0.5 of 30% = 15%
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"Total Samples: {len(X)}")
print(f"Training Samples (70%): {len(X_train)}")
print(f"Validation Samples (15%): {len(X_val)}")
print(f"Test Samples (15%): {len(X_test)}")

print("X_train shape:", X_train.shape)
print("X_val shape:", X_val.shape)
print("X_test shape:", X_test.shape)

"""# **3. Build and train an RNN/LSTM model.**"""

VOCAB_SIZE = min(MAX_WORDS, len(tokenizer.word_index) + 1)     # Vocabulary Size (Unique Tokens)
MAX_LEN = 102          # Maximum Sequence Length (MAX_LEN)
NUM_CLASSES = 52        # Total number of classes (Queues)
EMBEDDING_DIM = 100     # A common choice for embedding dimension
LSTM_UNITS = 128        # Number of units in the LSTM layer

# Training Hyperparameters
BATCH_SIZE = 64
EPOCHS = 10
# Training will stop early if validation loss plateaus.

model = Sequential([
    tf.keras.layers.Input(shape=(MAX_LEN,)),
    Embedding(VOCAB_SIZE, 200),
    Bidirectional(LSTM(128, return_sequences=True)),
    Dropout(0.3),
    Bidirectional(LSTM(64)),
    Dropout(0.3),
    Dense(256, activation="relu"),
    Dropout(0.3),
    Dense(NUM_CLASSES, activation="softmax")
])

model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

MODEL_PATH = "/content/drive/MyDrive/GUVI/best_lstm_model.keras"

checkpoint_cb = ModelCheckpoint(
    filepath=MODEL_PATH,
    monitor="val_accuracy",
    save_best_only=True,
    save_weights_only=False,
    verbose=1
)

early_stopping_cb = EarlyStopping(
    monitor="val_loss",
    patience=3,
    restore_best_weights=True,
    verbose=1
)

lr_cb = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=2,
    verbose=1
)

history = model.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=64,
    callbacks=[checkpoint_cb, early_stopping_cb, lr_cb],
    verbose=1
)

"""# **4. Evaluate model, tune hyperparameters.**"""

#  Evaluate on Test Data (Accuracy)
print("\nEvaluating Model on Test Data")

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)

print(f"Test Accuracy: {test_accuracy:.4f}")

# Precision, Recall, F1-Score

y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

print("\nClassification Report:")
print(
    classification_report(
        y_test,
        y_pred,
        target_names=label_encoder.classes_
    )
)

#Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

labels = label_encoder.classes_

plt.figure(figsize=(18, 16))
sns.heatmap(
    cm,
    xticklabels=labels,
    yticklabels=labels,
    cmap="Blues",
    cbar=True,
    fmt="d"
)

plt.title("Confusion Matrix with Queue Labels")
plt.xlabel("Predicted Queue")
plt.ylabel("Actual Queue")
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

#tune hyperparameters
def build_and_train_model(
    embedding_dim,
    lstm_units,
    batch_size
):
    model = Sequential([
        tf.keras.layers.Input(shape=(MAX_LEN,)),
        Embedding(VOCAB_SIZE, embedding_dim),
        Bidirectional(LSTM(lstm_units, return_sequences=True)),
        Dropout(0.3),
        Bidirectional(LSTM(lstm_units // 2)),
        Dropout(0.3),
        Dense(256, activation="relu"),
        Dropout(0.3),
        Dense(NUM_CLASSES, activation="softmax")
    ])

    model.compile(
        optimizer=Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )

    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        epochs=5,
        batch_size=batch_size,
        verbose=0
    )

    best_val_acc = max(history.history["val_accuracy"])
    return best_val_acc

embedding_dims = [100, 200]
lstm_units_list = [128, 256]
batch_sizes = [32, 64]

tuning_results = []

for emb in embedding_dims:
    for lstm_units in lstm_units_list:
        for batch in batch_sizes:
            val_acc = build_and_train_model(
                embedding_dim=emb,
                lstm_units=lstm_units,
                batch_size=batch
            )

            tuning_results.append({
                "embedding_dim": emb,
                "lstm_units": lstm_units,
                "batch_size": batch,
                "val_accuracy": val_acc
            })

            print(
                f"Embedding={emb}, LSTM={lstm_units}, "
                f"Batch={batch} -> Val Accuracy={val_acc:.4f}"
            )

tuning_df = pd.DataFrame(tuning_results)
tuning_df = tuning_df.sort_values("val_accuracy", ascending=False)

tuning_df

#Select best configuration
best_config = tuning_df.iloc[0]

BEST_EMBEDDING_DIM = int(best_config["embedding_dim"])
BEST_LSTM_UNITS = int(best_config["lstm_units"])
BEST_BATCH_SIZE = int(best_config["batch_size"])

print("Best Hyperparameters Selected:")
print(best_config)

#Train FINAL MODEL (used for Gemini integration)
final_model = Sequential([
    tf.keras.layers.Input(shape=(MAX_LEN,)),
    Embedding(VOCAB_SIZE, BEST_EMBEDDING_DIM),
    Bidirectional(LSTM(BEST_LSTM_UNITS, return_sequences=True)),
    Dropout(0.3),
    Bidirectional(LSTM(BEST_LSTM_UNITS // 2)),
    Dropout(0.3),
    Dense(256, activation="relu"),
    Dropout(0.3),
    Dense(NUM_CLASSES, activation="softmax")
])

final_model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history = final_model.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=BEST_BATCH_SIZE,
    callbacks=[checkpoint_cb, early_stopping_cb, lr_cb],
    verbose=1
)

#Save & load best model
final_model = tf.keras.models.load_model("/content/drive/MyDrive/GUVI/best_lstm_model.keras")

test_loss, test_accuracy = final_model.evaluate(
    X_test,
    y_test,
    verbose=0
)

print(f"Final Test Accuracy: {test_accuracy:.4f}")

y_pred_probs = final_model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

print(
    classification_report(
        y_test,
        y_pred,
        target_names=label_encoder.classes_
    )
)

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(18, 16))
sns.heatmap(
    cm,
    xticklabels=label_encoder.classes_,
    yticklabels=label_encoder.classes_,
    cmap="Blues",
    fmt="d"
)

plt.title("Confusion Matrix â€“ Final LSTM Model")
plt.xlabel("Predicted Queue")
plt.ylabel("Actual Queue")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

metrics = {
    "test_accuracy": test_accuracy
}

with open("/content/drive/MyDrive/GUVI/final_metrics.json", "w") as f:
    json.dump(metrics, f)

"""# **5. Integrate the Gemini API for automatic replies.**"""

from google import genai
import os


# --- Configuration and Initialization ---
os.environ['GEMINI_API_KEY'] = "AIzaSyB4oGttDVS3_EvID7B-9diFoHc7w2iy-Fc"
client = genai.Client()

def predict_queue(text):
    cleaned = clean_text(text)
    seq = tokenizer.texts_to_sequences([cleaned])
    padded = pad_sequences(seq, maxlen=MAX_LEN, padding="post")
    pred = final_model.predict(padded)
    label_index = np.argmax(pred)
    return label_encoder.inverse_transform([label_index])[0]

def generate_gemini_reply(ticket_text, predicted_queue):
    prompt = f"""
You are a professional customer support assistant.

Customer Ticket:
\"\"\"{ticket_text}\"\"\"

Predicted Department:
{predicted_queue}

Instructions:
- Write a short, polite, and empathetic reply.
- Acknowledge the customer's issue.
- Assure the customer that the issue will be handled.
- Do NOT include names, signatures, or placeholders.
- Do NOT include company name.
- End the reply professionally without sign-off.
"""
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt
    )
    return response.text.strip()

sample_ticket = df["body"].iloc[5]

print("Customer Ticket:\n", sample_ticket)
print("\nPredicted Queue:", predict_queue(sample_ticket))
print("\nGenerated Reply:\n")
print(generate_gemini_reply(sample_ticket, predict_queue(sample_ticket)))

user_ticket = input("Enter customer ticket: ")

predicted_queue = predict_queue(user_ticket)
reply = generate_gemini_reply(user_ticket, predicted_queue)

print("Predicted Queue:", predicted_queue)
print("\nGenerated Reply:\n", reply)

import pickle
import os

SAVE_DIR = "/content/drive/MyDrive/GUVI"
os.makedirs(SAVE_DIR, exist_ok=True)

with open(f"{SAVE_DIR}/tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

with open(f"{SAVE_DIR}/label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

print("Tokenizer and LabelEncoder saved successfully.")