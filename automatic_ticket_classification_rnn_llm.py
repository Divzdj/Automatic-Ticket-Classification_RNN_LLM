# -*- coding: utf-8 -*-
"""Automatic Ticket Classification_RNN_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4Ivr5hOmJeFQ_GRFMCcyr8XMC9Jxrdf
"""

#!pip install datasets pandas matplotlib

"""# **1.Dataset exploration**"""

from datasets import load_dataset
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, save_model,load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow. keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow. keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import re
import random
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np
import os
import json
import time
# Gemini API Imports
from google import genai
from google.genai import types

# Set a random seed for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# 1. Load the Dataset
print(" Loading the dataset: Tobi-Bueck/customer-support-tickets...")
try:
    ds = load_dataset("Tobi-Bueck/customer-support-tickets")
    print("Dataset loaded successfully.")
except Exception as e:
    print(f" Error loading dataset: {e}")
    # Stop execution if dataset fails to load
    exit()

# Convert the 'train' split to a pandas DataFrame for easier exploration
# Assuming 'train' is the primary and only split needed for exploration
df = ds['train'].to_pandas()

print("\n Basic Dataset Information ")
print(f"Total number of records in the 'train' split: {len(df)}")
print("\nFirst 5 rows of the DataFrame:")
# Display the input ('body') and target ('queue') fields
print(df[['body', 'queue']].head())

# 2. Explore Data Types
print("\nData Types of Key Fields ")
df.info()

#  3. Unique Queue Counts (Target Variable Analysis)
print("\nTarget Variable Analysis: 'queue' Distribution ")
queue_counts = df['queue'].value_counts()
num_classes = len(queue_counts)

print(f"Number of unique classes (queues): {num_classes}")
print("\nTop 10 Queue Distribution:")
print(queue_counts.head(10))

# Visualize the class distribution to check for imbalance
plt.figure(figsize=(12, 6))
queue_counts.plot(kind='bar', color='skyblue')
plt.title('Distribution of Customer Support Ticket Queues (Target Classes)')
plt.xlabel('Queue / Department')
plt.ylabel('Number of Tickets')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# 4. Text Length Distribution (Input Variable Analysis)
print("\n Input Variable Analysis: 'body' Text Length ")

# Calculate the length of each ticket body in terms of words
df['body_length'] = df['body'].apply(lambda x: len(str(x).split()))

# Get descriptive statistics
length_stats = df['body_length'].describe(percentiles=[.50, .75, .90, .95, .99])
print("Body Text Length (in number of words) Statistics:")
print(length_stats)

# Visualize the text length distribution
plt.figure(figsize=(10, 5))
plt.hist(df['body_length'], bins=50, color='lightcoral', edgecolor='black', range=(0, length_stats['99%'] if length_stats['99%'] < 500 else 500)) # Cap at 500 or 99th percentile for better visualization
plt.title('Distribution of Ticket Body Lengths (in words)')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')

# Mark the 95th percentile (a common choice for MAX_LEN)
max_len_95 = int(length_stats['95%'])
plt.axvline(max_len_95, color='red', linestyle='dashed', linewidth=2, label=f'95th Percentile ({max_len_95} words)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""# **2.Preprocessing, label encoding.**"""

# Text Preprocessing (Cleaning and Lowercasing)

def clean_text(text):
    """Simple function to clean the text data."""
    text = str(text).lower()
    # UPDATED REGEX: Preserves common diacritics for multilingual data (e.g., German, French)
    # The new character set is: [a-z] + spaces (\s) + common diacritics/special letters.
    text = re.sub(r'[^a-z\s√°√©√≠√≥√∫√§√´√Ø√∂√º√°√†√¢√®√©√™√´√¨√≠√Æ√Ø√≤√≥√¥√π√∫√ª√±√ß√ü]', '', text)
    # Remove extra whitespace and strip
    text = re.sub(r'\s+', ' ', text).strip()
    return text

print(" Text Preprocessing ")
df['cleaned_body'] = df['body'].apply(clean_text)

# Check a sample
sample_index = 0
print("Original Text:", df['body'].iloc[sample_index])
print("Cleaned Text:", df['cleaned_body'].iloc[sample_index])

# Label Encoding for Queue

print("\nLabel Encoding for Queue ")
# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the 'queue' column
df['encoded_queue'] = label_encoder.fit_transform(df['queue'])

# Save the mapping for decoding predictions later
QUEUE_MAPPING = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
NUM_CLASSES = len(label_encoder.classes_)

print(f"Total number of classes: {NUM_CLASSES}")
print("Label to Index Mapping Sample:", list(QUEUE_MAPPING.items())[:5])

# Tokenization and Sequence Preparation

print("\nTokenization and Padding/Truncation ")


MAX_WORDS = 15000  # Vocabulary size: consider the top 15,000 most frequent words
MAX_LEN = 150      # Sequence length: set to ~95th percentile word count (e.g., 150)

# Initialize Keras Tokenizer
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
# Fit on the cleaned text data
tokenizer.fit_on_texts(df['cleaned_body'])

# Convert text to sequences of integers
sequences = tokenizer.texts_to_sequences(df['cleaned_body'])

# Pad/Truncate sequences to MAX_LEN
X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')
y = df['encoded_queue'].values # The target array

# Save the tokenizer and key parameters for Day 3
VOCAB_SIZE = len(tokenizer.word_index) + 1 # +1 for the 0 padding

print(f"Vocabulary Size (Unique Tokens): {VOCAB_SIZE}")
print(f"Maximum Sequence Length (MAX_LEN): {MAX_LEN}")
print("Shape of Final Input (X):", X.shape)

# Split Data into Train, Validation, and Test Sets

print("\nData Splitting (70/15/15) ")

# 1. Split into Training (70%) and Temporary Test (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

# 2. Split the Temporary Test (30%) into Validation (15%) and Final Test (15%)
# 0.5 of 30% = 15%
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

# Convert labels to one-hot encoding if needed for model (RNN generally uses sparse_categorical_crossentropy
# for integer labels, but we'll leave it as integers for now. If using an activation other than softmax, or
# a different loss, we'd use this:
# y_train = tf.keras.utils.to_categorical(y_train, num_classes=NUM_CLASSES)
# y_val = tf.keras.utils.to_categorical(y_val, num_classes=NUM_CLASSES)
# y_test = tf.keras.utils.to_categorical(y_test, num_classes=NUM_CLASSES)

print(f"Total Samples: {len(X)}")
print(f"Training Samples (70%): {len(X_train)}")
print(f"Validation Samples (15%): {len(X_val)}")
print(f"Test Samples (15%): {len(X_test)}")

"""# **3. Build and train an RNN/LSTM model.**"""

VOCAB_SIZE = 50393      # Vocabulary Size (Unique Tokens)
MAX_LEN = 150           # Maximum Sequence Length (MAX_LEN)
NUM_CLASSES = 52        # Total number of classes (Queues)
EMBEDDING_DIM = 100     # A common choice for embedding dimension
LSTM_UNITS = 128        # Number of units in the LSTM layer

# Training Hyperparameters
BATCH_SIZE = 64
EPOCHS = 10
# Training will stop early if validation loss plateaus.

def build_lstm_model(vocab_size, max_len, embedding_dim, lstm_units, num_classes):
    """
    Builds a Many-to-One Sequential LSTM model for text classification.
    """
    model = Sequential([
        # 1. Embedding Layer
        # Takes the integer-encoded sequence (input_length=MAX_LEN) and converts
        # each index into a dense vector (output_dim=EMBEDDING_DIM).
        Embedding(
            input_dim=vocab_size,
            output_dim=embedding_dim,
            input_length=max_len,
            # Using mask_zero=True is highly recommended with padding
            mask_zero=True
        ),

        # 2. LSTM Layer
        # Processes the sequence of embeddings. Since it's Many-to-One classification,
        # we set return_sequences=False to get only the final output state.
        LSTM(lstm_units),

        # 3. Regularization/Dropout
        Dropout(0.5),

        # 4. Dense Output Layer
        # Outputs a probability distribution over all 52 classes using softmax.
        Dense(num_classes, activation='softmax')
    ])

    return model

# Instantiate and summarize the model
model = build_lstm_model(VOCAB_SIZE, MAX_LEN, EMBEDDING_DIM, LSTM_UNITS, NUM_CLASSES)
model.summary()

# Compilation
model.compile(
    optimizer='adam',
    # Loss: SparseCategoricalCrossentropy is used because y_train labels are
    # integers (0 to 51) and NOT one-hot encoded.
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

#  Define Callbacks
# EarlyStopping: Monitor validation loss and stop if it doesn't improve for 3 epochs.
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# ModelCheckpoint: Save the model only when validation loss is at its minimum.
model_checkpoint = ModelCheckpoint(
    'ticket_classifier_lstm.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

#  Training Execution
print("\n--- Starting Baseline Training ---")
print(f"Training on {43235} samples, Validating on {9265} samples.")

history = model.fit(
    X_train, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
 )

print("\n--- Baseline Training Status ---")
print(f"Model successfully built and compiled.")
print("Basic RNN/LSTM model defined and ready for initial training.")

"""# **4. Evaluate model, tune hyperparameters.**"""

#  1. Constants
VOCAB_SIZE = 50419      # Final observed Vocabulary Size
NUM_CLASSES = 52        # Total number of classes
MAX_LEN = 150           # Sequence Length
EMBEDDING_DIM = 200     # Tuned parameter (increased from 100)
GRU_UNITS = 128
DROPOUT_RATE = 0.4
BATCH_SIZE = 64
EPOCHS = 15             # Increased epochs, relying on EarlyStopping

#  2. Model Definition
def build_bidirectional_gru_model(vocab_size, embedding_dim, gru_units, num_classes, dropout_rate):
    """
    Builds the optimized Bidirectional GRU model for hyperparameter tuning.
    """
    model = Sequential([
        # Embedding Layer: input_dim=50419, output_dim=200 (Tuned)
        Embedding(
            input_dim=vocab_size,
            output_dim=embedding_dim,
            mask_zero=True
        ),
        # Bidirectional GRU: Captures context from both directions
        Bidirectional(
            GRU(gru_units)
        ),
        Dropout(dropout_rate),
        # Dense Output Layer (52 classes)
        Dense(num_classes, activation='softmax')
    ])
    return model

# 3. Instantiate and Compile Model
tuned_model = build_bidirectional_gru_model(VOCAB_SIZE, EMBEDDING_DIM, GRU_UNITS, NUM_CLASSES, DROPOUT_RATE)

tuned_model.compile(
    optimizer='adam',
    # Loss for integer-encoded labels
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# 4. Define Callbacks
# EarlyStopping: Patience set to 4 to allow longer training runs for the better model
early_stopping_tuned = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)
model_checkpoint_tuned = ModelCheckpoint(
    # Save file with "_clean" to reflect the high-quality data used
    'ticket_classifier_gru_tuned_clean.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# 5. Training Execution
# Note: X_train, y_train, X_val, y_val must be globally defined from the splitting step.
print("\nStarting FINAL Training for Tuned Bidirectional GRU Model ")

history_tuned = tuned_model.fit(
    X_train, y_train,
    epochs=EPOCHS, # 15
    batch_size=BATCH_SIZE, # 64
    validation_data=(X_val, y_val),
    callbacks=[early_stopping_tuned, model_checkpoint_tuned],
    verbose=1
)

print("\nTraining Complete ")
print("Best model weights saved.")
save_model(model, 'ticket_classifier_gru_tuned_clean.keras')

# Load the Best Tuned Model
# Loads the weights saved at Epoch 4 (best val_loss=1.4964)
MODEL_PATH = 'ticket_classifier_gru_tuned_clean.keras'
try:
    best_model = load_model(MODEL_PATH)
    print(f"Successfully loaded best-tuned GRU model from: {MODEL_PATH}")
except Exception as e:
    print(f"Error loading model: {e}. Cannot proceed with evaluation.")
    # Exit or handle error if the file is not found
    exit()

print("\n Making Predictions on Unseen Test Set ")
# X_test and y_test must be defined from the final data splitting step
y_pred_probs = best_model.predict(X_test, verbose=1)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = y_test

#  Final Model Classification Report
print("\n Final Model Classification Report (Unseen Test Data) ")
# target_names is needed for the report, using the classes from your LabelEncoder
# label_encoder must be defined from the preprocessing step
target_names = label_encoder.classes_
report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0)
print(report)

overall_accuracy = accuracy_score(y_true, y_pred)
print(f"\nOverall Test Set Accuracy: {overall_accuracy:.4f}")

#  Confusion Matrix Analysis (Visual)
print("\n Visualizing Normalized Confusion Matrix ")
cm = confusion_matrix(y_true, y_pred)
# Normalize the matrix to show the percentage of true labels correctly predicted (Recall)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(16, 14))
sns.heatmap(
    cm_normalized,
    annot=False,
    fmt='.2f',
    cmap='Blues',
    xticklabels=target_names,
    yticklabels=target_names
)
plt.title(f'Normalized Confusion Matrix (Final Test Accuracy: {overall_accuracy:.2%})', fontsize=16)
plt.ylabel('True Queue Label')
plt.xlabel('Predicted Queue Label')
plt.xticks(rotation=90, ha='center', fontsize=6)
plt.yticks(rotation=0, fontsize=6)
plt.tight_layout()
plt.show()

"""# **5. Integrate the Gemini API for automatic replies.**"""

import os
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
# NOTE: Need to suppress NameErrors if these were not globally imported earlier
try:
    from google import genai
    from google.genai import types
    from google.genai import Client
    from sklearn.preprocessing import LabelEncoder
    from IPython.display import display, HTML
except ImportError:
    print("Warning: Some libraries (google.genai, IPython) might be missing. Using local mocks.")


# --- Mock Dependencies for Runnable Demo ---
# These mocks ensure the classification step does not crash if prior cell setup failed.
class MockLabelEncoder:
    def __init__(self, classes): self.classes_ = np.array(classes)
    def inverse_transform(self, indices): return self.classes_[indices]
    def transform(self, classes):
        # Simple implementation for necessary transform calls
        class_map = {name: i for i, name in enumerate(self.classes_)}
        return np.array([class_map.get(c, -1) for c in classes])

def mock_clean_text(text):
    return text.lower().strip()

# Mock objects if they don't exist
try:
    clean_text
except NameError:
    clean_text = mock_clean_text

# MOCK: Dummy objects for classification helper to run
class MockModel:
    def predict(self, X_new, verbose=0):
        # Mocks a prediction of 97.16% for index 6 ('Billing and Payments')
        probs = np.zeros((X_new.shape[0], 52))
        probs[:, 6] = 0.9716
        return probs

class MockTokenizer:
    def texts_to_sequences(self, texts): return [[1, 2, 3]]

MOCK_CLASSES = [
    'Arts & Entertainment/Movies', 'Arts & Entertainment/Music', 'Autos & Vehicles/Maintenance',
    'Autos & Vehicles/Sales', 'Beauty & Fitness/Cosmetics', 'Beauty & Fitness/Fitness Training',
    'Billing and Payments', 'Books & Literature/Fiction', 'Books & Literature/Non-Fiction',
    'Business & Industrial/Manufacturing', 'Customer Service', 'Finance/Investments',
    'Finance/Personal Finance', 'Food & Drink/Groceries', 'Food & Drink/Restaurants', 'Games',
    'General Inquiry', 'Health/Medical Services', 'Health/Mental Health', 'Hobbies & Leisure/Collectibles',
    'Hobbies & Leisure/Crafts', 'Home & Garden/Home Improvement', 'Home & Garden/Landscaping',
    'Human Resources', 'IT & Technology/Hardware Support', 'IT & Technology/Network Infrastructure',
    'IT & Technology/Security Operations', 'IT & Technology/Software Development', 'IT Support',
    'Jobs & Education/Online Courses', 'Jobs & Education/Recruitment', 'Law & Government/Government Services',
    'Law & Government/Legal Advice', 'News', 'Online Communities/Forums', 'Online Communities/Social Networks',
    'People & Society/Culture & Society', 'Pets & Animals/Pet Services', 'Pets & Animals/Veterinary Care',
    'Product Support', 'Real Estate', 'Returns and Exchanges', 'Sales and Pre-Sales',
    'Science/Environmental Science', 'Science/Research', 'Service Outages and Maintenance',
    'Shopping/E-commerce', 'Shopping/Retail Stores', 'Sports', 'Technical Support',
    'Travel & Transportation/Air Travel', 'Travel & Transportation/Land Travel'
]

try:
    best_model
except NameError:
    best_model = MockModel()
    tokenizer = MockTokenizer()
    label_encoder = MockLabelEncoder(MOCK_CLASSES)

# --- Configuration and Initialization ---
os.environ['GEMINI_API_KEY'] = "masked_for_security"  # Replace with your actual API key
MAX_LEN = 150
GEMINI_MODEL = 'gemini-2.5-flash'
API_KEY = os.getenv("GEMINI_API_KEY", "")

# --- 1. Client Initialization ---
gemini_client = None
if not API_KEY:
    print("Error: API Key is empty. Cannot initialize Gemini Client.")
else:
    try:
        gemini_client = Client(api_key=API_KEY)
        print(f"Gemini Client Successfully Initialized. Client Type: {type(gemini_client)}")
    except Exception as e:
        print(f"Error initializing Gemini Client: {e}")

target_names = label_encoder.classes_
print(f"Using target queues: {list(target_names)}")


# --- 2. Core Function: Generative AI Reply (Maximum Resilience) ---

def generate_automated_reply(client, ticket_text, predicted_queue):
    """
    Uses the Gemini API to generate a professional, contextual reply.
    Uses the absolute minimum system instruction for maximum reliability against filters.
    """
    if client is None:
        return "Gemini API client is not available (Initialization failed). Cannot generate automated reply."

    # --- FINAL CORRECTION: Minimal System Instruction ---
    # Setting a simple persona is the most reliable system instruction.
    system_instruction = (
        "You are a professional, polite, and empathetic Tier 1 customer support agent."
    )

    # --- All formatting constraints moved to the main prompt ---
    # Asking for a 'Dear Customer' format ensures it is conversational.
    prompt = (
        f"The customer submitted the following support ticket:\n\n---\n{ticket_text}\n---"
        f"\n\nDraft a concise first-response email starting with 'Dear Customer,'. Do NOT try to solve the problem. Your reply must acknowledge the issue, thank them for their patience, and state that the ticket has been routed to the **{predicted_queue}** team for follow-up within 24 hours."
    )

    # --- Exponential Backoff Logic ---
    for attempt in range(5):
        try:
            # Assumes 'types' is correctly imported/defined in the environment
            config = types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.3,
                max_output_tokens=300
            )

            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=config
            )

            if response.text and response.text.strip():
                return response.text
            else:
                print(f"Warning: Gemini API returned empty content (Attempt {attempt+1}). Check safety settings or adjust prompt.")
                if attempt == 4:
                    return "Automatic reply failed: API returned empty content after 5 attempts. The prompt is likely still being filtered."
                time.sleep(2 ** attempt)

        except Exception as e:
            if attempt < 4:
                wait_time = 2 ** attempt
                print(f"API call failed (Attempt {attempt+1}). Retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"Final error after retries: {e}")
                return f"Automatic reply generation failed after multiple retries: {e}"
    return "Failed to generate reply due to API error."


# --- Classification Helper Function ---
def classify_ticket(ticket_text, classifier_model, tokenizer, label_encoder, max_len):
    """Tokenizes, pads, and classifies a ticket text."""
    # This uses the mock/real objects defined above
    cleaned_text = clean_text(ticket_text)
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    X_new = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')
    y_pred_probs = classifier_model.predict(X_new, verbose=0)
    predicted_index = np.argmax(y_pred_probs, axis=1)[0]
    predicted_queue = label_encoder.inverse_transform([predicted_index])[0]
    confidence = y_pred_probs[0][predicted_index]
    return predicted_queue, confidence

# --- 3. Test the End-to-End System ---
print("\n--- End-to-End System Demonstration ---")

NEW_TICKET = (
    "Hello Support Team,\n\nI just received my monthly bill and the amount is "
    "significantly higher than last month. I was charged for a premium feature "
    "I specifically canceled three months ago. Please review my account and "
    "correct the charge immediately. I have attached the previous invoice for reference.\n\n"
    "Account ID: 987654321"
)

# 3.1. Classify the new ticket
print("\nStep 1: Classifying Ticket...")
# Using the defined objects/mocks
predicted_queue, confidence = classify_ticket(NEW_TICKET, best_model, tokenizer, label_encoder, MAX_LEN)
print(f"-> Model Predicted Queue: **{predicted_queue}** (Confidence: {confidence:.2%})")

# 3.2. Generate the automated reply
print("\nStep 2: Generating Automated Reply via Gemini...")
automated_reply = generate_automated_reply(gemini_client, NEW_TICKET, predicted_queue)

# 3.3. Display Results
print("\n" + "="*50)
print(f"   AUTOMATED TICKET CLASSIFICATION & REPLY   ")
print("="*50)
print(f"**TICKET CLASSIFICATION:** {predicted_queue} ({confidence:.2%})")
print("\n**GENERATED AUTOMATED REPLY:**\n")
print(automated_reply)
print("="*50)

"""# **6.Pipeline Integration & Prototyping**"""

import time
from google.genai import types # Ensure this is imported globally

# --- A. Ticket Classification Function ---

def classify_ticket(ticket_text, classifier_model, tokenizer, label_encoder, max_len):
    """
    Classifies a raw ticket text using the loaded RNN model.
    NOTE: This uses the 'lime_model' which is built to be CPU/masking compatible.

    Assumes: clean_text, pad_sequences, np are available globally.
    """
    # 1. Preprocess
    cleaned_text = clean_text(ticket_text)
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    X_new = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')

    # 2. Predict
    # Use the LIME-compatible model for prediction
    y_pred_probs = classifier_model.predict(X_new, verbose=0)

    # 3. Decode
    predicted_index = np.argmax(y_pred_probs, axis=1)[0]
    predicted_queue = label_encoder.inverse_transform([predicted_index])[0]
    confidence = y_pred_probs[0][predicted_index]

    return predicted_queue, confidence


# --- B. Generative AI Reply Function (Highly Resilient) ---

def generate_automated_reply(client, ticket_text, predicted_queue):
    """
    Uses the Gemini API to generate a professional, contextual reply.
    """
    # NOTE: Ensure gemini_client is initialized before calling this function.
    GEMINI_MODEL = 'gemini-2.5-flash'

    if client is None:
        return "Gemini API client is not available. Cannot generate automated reply."

    # Minimal system instruction for maximum reliability
    system_instruction = (
        "You are a professional, polite, and empathetic Tier 1 customer support agent."
    )

    # Prompt clearly instructs the model on format and action
    prompt = (
        f"The customer submitted the following support ticket:\n\n---\n{ticket_text}\n---"
        f"\n\nDraft a concise first-response email starting with 'Dear Customer,'. Do NOT try to solve the problem. Your reply must acknowledge the issue, thank them for their patience, and state that the ticket has been routed to the **{predicted_queue}** team for follow-up within 24 hours."
    )

    for attempt in range(5):
        try:
            config = types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.3,
                max_output_tokens=300
            )

            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=config
            )

            if response.text and response.text.strip():
                return response.text
            else:
                if attempt == 4:
                    return "Automatic reply failed: API returned empty content after 5 attempts."
                time.sleep(2 ** attempt)

        except Exception as e:
            if attempt == 4:
                return f"Automatic reply generation failed after multiple retries: {e}"
            time.sleep(2 ** attempt)

    return "Failed to generate reply due to API error."

# Assuming the following are defined globally from previous steps:
# best_model (The MockModel placeholder for the classifier)
# tokenizer
# label_encoder
# gemini_client (the initialized google.genai.Client)
# MAX_LEN

# --- C. Sample Tickets for Testing ---
SAMPLE_TICKETS = [
    # Billing and Payments
    "I was charged twice for my subscription this month. I need a refund for the extra $19.99 immediately. Please check my statement and correct the invoice.",

    # Technical Support/IT Support
    "My internet connection keeps dropping every hour. I've tried restarting the router and my computer, but the problem persists. I can't work without a stable connection!",

    # Returns and Exchanges
    "The dress I ordered arrived damaged. The zipper is completely broken. I need to know the procedure for returning it and getting a replacement size medium.",

    # General Inquiry
    "What are your business hours during the holiday season? Specifically, will your customer service line be open on Christmas Day?",

    # Account Services (Hypothetical)
    "I moved last week and need to update the primary email address and mailing address associated with my account. The current address is outdated."
]

# --- D. End-to-End Pipeline Execution ---

print("=" * 80)
print("             üöÄ END-TO-END TICKET AUTOMATION PIPELINE DEMO üöÄ")
print("=" * 80)

results = []

for i, ticket_text in enumerate(SAMPLE_TICKETS):
    print(f"\n--- Processing Ticket {i+1}/{len(SAMPLE_TICKETS)} ---")
    print(f"INPUT: {ticket_text[:75]}...")

    # 1. CLASSIFICATION (RNN/LSTM Model)
    try:
        predicted_queue, confidence = classify_ticket(
            ticket_text,
            # FIX: Use 'best_model' (the defined mock) instead of 'lime_model'
            best_model,
            tokenizer,
            label_encoder,
            MAX_LEN
        )
        print(f"‚úÖ CLASSIFIED: {predicted_queue} (Confidence: {confidence:.2%})")
    except Exception as e:
        predicted_queue = "CLASSIFICATION FAILED"
        confidence = 0.0
        print(f"‚ùå CLASSIFICATION ERROR: {e}")

    # 2. REPLY GENERATION (Gemini API)
    if predicted_queue != "CLASSIFICATION FAILED":
        automated_reply = generate_automated_reply(
            gemini_client,
            ticket_text,
            predicted_queue
        )
        print("‚úÖ REPLY GENERATED.")
    else:
        automated_reply = "Cannot generate reply due to classification failure."

    results.append({
        "Ticket Text": ticket_text,
        "Predicted Queue": predicted_queue,
        "Confidence": confidence,
        "Automated Reply": automated_reply
    })

    print("-" * 30)
    print(f"Reply Preview:\n{automated_reply.splitlines()[0]}...")
    print("-" * 30)

print("\n" + "=" * 80)
print("MANUAL EVALUATION REQUIRED: Review the replies above for politeness and routing accuracy.")
print("Note: Since you are using the mock model, the prediction will be the same for all tickets.")
print("=" * 80)